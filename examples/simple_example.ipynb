{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Running cytoself in Colab\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/royerlab/cytoself/blob/main/examples/simple_example.ipynb)\n",
    "## Introduction\n",
    "This jupyter notebook shows a simple example of how to use *cytoself* with a few example images and pre-trained model.\n",
    "The pre-trained model was trained with the image shape of (100, 100, 2) in which the channel consists of fluorescence\n",
    "protein and nucleus distance.\n",
    "\n",
    "## Example demo\n",
    "Let's get started with a simple example.\n",
    "It is highly recommended to use GPU if you can.\n",
    "\n",
    "Note: In case an error occurs, which is observed occasionally, please be patient and try to run the cell again.\n",
    "If the error persists, please try to restart the runtime.\n",
    "\n",
    "First, install dependencies and configure tensorflow version to 1.15.2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Set tensorflow to 1.15.2\n",
    "%tensorflow_version 1.x\n",
    "\n",
    "# Install requirement packages\n",
    "!pip install git+https://github.com/royerlab/cytoself.git\n",
    "!pip uninstall -y matplotlib\n",
    "!pip install matplotlib==3.1.3  # for saving png\n",
    "!pip uninstall -y h5py\n",
    "!pip install h5py==2.10.0  # for loading pre-trained model\n",
    "# !pip install git+https://github.com/royerlab/cytoself"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "(Optional) You can use wandb to monitor your computation resources.\n",
    "Please note that this requires account sign up in wandb."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install wandb\n",
    "# import wandb\n",
    "# wandb.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import dependencies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gdown\n",
    "from cytoself.data_loader.data_manager import DataManager\n",
    "from cytoself.models import CytoselfFullModel\n",
    "from cytoself.data_loader.data_generator import image_and_label_generator\n",
    "from cytoself.analysis.analytics import Analytics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's download example data and trained model weights.\n",
    "Here we only download a fraction of test data that was used in our preprint for the sake of time and space."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Download model weights (full model)\n",
    "gdown.download(\n",
    "    'https://drive.google.com/uc?id=1gkiEMKdadOel4Xh6KoS2U603JTkZhgDw',\n",
    "    'pretrained_model.h5',\n",
    "    quiet=True\n",
    ")\n",
    "# Download label data\n",
    "gdown.download(\n",
    "    'https://drive.google.com/uc?id=16-0bhKrUMbZ0DSz768Z_q13yNivHyfVO',\n",
    "    'example_label.npy',\n",
    "    quiet=True\n",
    ")\n",
    "# Download image data\n",
    "gdown.download(\n",
    "    'https://drive.google.com/uc?id=1znRLbYJJqd11Zqv-5_yUmNjarKcwIWMg',\n",
    "    'example_image.npy',\n",
    "    quiet=True\n",
    ")\n",
    "# Download localization table\n",
    "gdown.download(\n",
    "    'https://drive.google.com/uc?id=1RM654Qavcy8gG5uy3mCzi8EsOT_xOlVd',\n",
    "    'protein_uniloc.csv',\n",
    "    quiet=True\n",
    ")\n",
    "\n",
    "# Download dendrogram index to plot feature spectrum\n",
    "gdown.download(\n",
    "    'https://drive.google.com/uc?id=1WrxhGsSzivZVAlL_K2FLVsRmHrsfhyrI',\n",
    "    'dgram_index1.npy',\n",
    "    quiet=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's load example data. The image and label data consist of proteins with unique localization.\n",
    "The localization table indicates the unique localization for each protein."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "image_data = np.load('example_image.npy')\n",
    "label_data = np.load('example_label.npy', allow_pickle=True)\n",
    "gt_table = pd.read_csv('protein_uniloc.csv')\n",
    "\n",
    "# The image data has 3 channels which are protein label, nucleus and nucleus distance.\n",
    "# In this example we only use protein label and nucleus distance channels.\n",
    "image_data = image_data[:, ..., [0, 2]]\n",
    "\n",
    "# Make sure that the label data has 2 dimensions.\n",
    "label_data = label_data.reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create a cytoself model object.\n",
    "This is an object wrapping Tensorflow Model object and some convenient auxiliary functions\n",
    "(e.g. extracting latent representations)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = CytoselfFullModel(input_image_shape=[100, 100, 2], num_fc_output_classes=len(np.unique(label_data)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Consolidate datasets with DataManager. In this example we use the same data for train, validation and test data.\n",
    "Then compile the model.\n",
    "\n",
    "Note: The data split here is only to provide an example of how to run cytoself. Please make sure the data is\n",
    "split properly when you train your read data.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_manager = DataManager(\n",
    "        train_data=image_data[:100],\n",
    "        val_data=image_data[100:200],\n",
    "        test_data=image_data[200:],\n",
    "        train_label=label_data[:100],\n",
    "        val_label=label_data[100:200],\n",
    "        test_label=label_data[200:],\n",
    ")\n",
    "\n",
    "# Compile the model with data_manager\n",
    "model.compile_with_datamanager(data_manager)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready to start training cytoself.\n",
    "`train_with_datamanager` is a convenient method with very few arguments but there are other methods with more flexibility.\n",
    "Please check the codebase.\n",
    "\n",
    "You can change batch size depending on the available GPU memory.\n",
    "Training histories will be displayed after training. (Make sure the version of matplotlib==3.1.3.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train_with_datamanager(data_manager, batch_size=64, max_epoch=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training is done, but that was only for demonstration purpose.\n",
    "We can load a pre-trained model to pretend we have a well-trained the model if all parameters are in the default setting.\n",
    "(Make sure the version of h5py==2.10.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.load_model('pretrained_model.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use `Analytics` module to perform various analysis.\n",
    "For example, we use `Analytics` to compute the UMAP of global representation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analytics = Analytics(model, data_manager, gt_table)\n",
    "analytics.calc_plot_umaps_gt(\"vec\", titles=\"Unique localization\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's compute a feature spectrum from local representation.\n",
    "We first compute a cluster map against vq index.\n",
    "The cluster map combines similar vq indices so that we obtain an order of vq index where similar features come close to each other.\n",
    "We will use this ordering to plot feature spectrum."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analytics.plot_clustermaps()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The index order of the dendrogram in the clustermap can be saved and reload so that you don't need to compute the cluster map everytime you want to get feature spectra.\n",
    "Here we load a pre-computed dendrogram index that was computed with all test data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analytics.load_dendrogram_index('dgram_index1.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's plot a feature spectrum."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analytics.plot_feature_spectrum_from_image(image_data[:1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we have walked through the basics about how to get\n",
    "localization clustering and feature spectrum using example data.\n",
    "Most of the intermediate computation results can be saved so that you can start downstream analysis from the middle.\n",
    "Please check the codebase for what arguments are available for the moment.\n",
    "An API guide will available in the future."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}